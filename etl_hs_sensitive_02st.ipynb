{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMAyGSq4FCFbrT9JAL1G7Ls",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidP0011/etl_functions/blob/main/etl_hs_sensitive_02st.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# INICIALIZACI√ìN"
      ],
      "metadata": {
        "id": "SV1jWDmAcEBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title INSTALACI√ìN DE REPOSITORIO DE FUNCIONES PY\n",
        "!pip install --force-reinstall git+https://github.com/DavidP0011/functions_for_notebooks@main\n"
      ],
      "metadata": {
        "id": "IOXVVtB8cJFu",
        "outputId": "305423a9-9ee9-49ba-d451-c4ca7062f429",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/DavidP0011/functions_for_notebooks@main\n",
            "  Cloning https://github.com/DavidP0011/functions_for_notebooks (to revision main) to /tmp/pip-req-build-plneta6u\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/DavidP0011/functions_for_notebooks /tmp/pip-req-build-plneta6u\n",
            "  Resolved https://github.com/DavidP0011/functions_for_notebooks to commit b1f35391ea69c67e0c73f34d1bfdea454f46a8a9\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: funcones-py-notebooks\n",
            "  Building wheel for funcones-py-notebooks (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for funcones-py-notebooks: filename=funcones_py_notebooks-0.0.0-py3-none-any.whl size=70177 sha256=3f3f9975b743e7761fd6d5b5b56597f6b7ce9b55bb92effe864d731cef02f40b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-unogijl3/wheels/e4/da/f0/65c77a4a07075343aef399e3b22a17e46b2968e2b120cb9020\n",
            "Successfully built funcones-py-notebooks\n",
            "Installing collected packages: funcones-py-notebooks\n",
            "Successfully installed funcones-py-notebooks-0.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title IDENTIFICACION DE ENTORNO, INSTALACI√ìN GOOGLE DRIVE\n",
        "\n",
        "from common.dpm_GCP_ini import ini_environment_identification, ini_google_drive_instalation\n",
        "\n",
        "# Detectar el entorno de ejecuci√≥n\n",
        "ini_environment_identificated = ini_environment_identification()\n",
        "print(f\"[INFO ‚ÑπÔ∏è] Entorno detectado: {ini_environment_identificated}\", flush=True)\n",
        "\n",
        "# Montar Google Drive si entorno_identificado_str es Colab\n",
        "params = {\"entorno_identificado_str\": ini_environment_identificated}\n",
        "ini_google_drive_instalation(params)\n",
        "\n",
        "# Declarar las rutas de las credenciales\n",
        "GCP_json_keyfile_local = r\"C:/api_keys/XXX.json\"\n",
        "GCP_json_keyfile_colab = \"/content/drive/MyDrive/ANIMUM DIRECCION/DIRECCION BI/NOTEBOOKS/api_keys/animum-dev-datawarehouse-google-colab.json\"\n",
        "GCP_json_keyfile_GCP_secret_id = \"notebook-vm\""
      ],
      "metadata": {
        "id": "iDbd6lvw3wOq",
        "outputId": "6df1baf3-833c-4a16-97d1-f390f86954d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO ‚ÑπÔ∏è] Entorno detectado: COLAB\n",
            "Mounted at /content/drive\n",
            "[INFO ‚ÑπÔ∏è] Google Drive montado correctamente.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title secrets_as_os_environ()\n",
        "\n",
        "# __________________________________________________________________________________________________________________________________________________________\n",
        "# secrets_get_dic\n",
        "# __________________________________________________________________________________________________________________________________________________________\n",
        "def secrets_get_dic(config: dict) -> dict:\n",
        "    \"\"\"\n",
        "    Recupera secretos desde Secret Manager y los devuelve en un diccionario {secret_id: valor}.\n",
        "    No toca os.environ.\n",
        "\n",
        "    Args:\n",
        "        config (dict):\n",
        "            - project_id (str): ID del proyecto GCP.\n",
        "            - secrets_list (list[str]): Lista de IDs de secretos a leer.\n",
        "            - ini_environment_identificated (str, opcional): \"LOCAL\" | \"COLAB\" | \"COLAB_ENTERPRISE\" | <otro>.\n",
        "            - json_keyfile_local (str, opcional): Ruta JSON si entorno LOCAL.\n",
        "            - json_keyfile_colab (str, opcional): Ruta JSON si entorno COLAB.\n",
        "            - json_keyfile_GCP_secret_id (str, opcional): SecretId con la JSON key si entorno GCP.\n",
        "\n",
        "    Returns:\n",
        "        dict: Mapa {secret_id: secret_value}.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: Si faltan par√°metros obligatorios o tipos inv√°lidos.\n",
        "        Exception: Si falla el acceso a Secret Manager.\n",
        "    \"\"\"\n",
        "    import time\n",
        "    from google.cloud import secretmanager\n",
        "\n",
        "    project_id_str = config.get(\"project_id\")\n",
        "    secrets_list = config.get(\"secrets_list\")\n",
        "    if not project_id_str:\n",
        "        raise ValueError(\"[VALIDATION [ERROR ‚ùå]] Falta 'project_id' en config.\")\n",
        "    if not secrets_list or not isinstance(secrets_list, list):\n",
        "        raise ValueError(\"[VALIDATION [ERROR ‚ùå]] Falta 'secrets_list' (list) en config.\")\n",
        "\n",
        "    print(\"üîπüîπüîπ [START ‚ñ∂Ô∏è] Lectura de secretos (in-memory) üîπüîπüîπ\", flush=True)\n",
        "    t0 = time.time()\n",
        "\n",
        "    try:\n",
        "        credentials = _ini_authenticate_API(config, project_id_str)\n",
        "        client_sm = secretmanager.SecretManagerServiceClient(credentials=credentials)\n",
        "        print(\"[AUTHENTICATION SUCCESS ‚úÖ] Credenciales obtenidas.\", flush=True)\n",
        "\n",
        "        secrets_dic = {}\n",
        "        warnings_int = 0\n",
        "        for secret_id in secrets_list:\n",
        "            try:\n",
        "                name = f\"projects/{project_id_str}/secrets/{secret_id}/versions/latest\"\n",
        "                response = client_sm.access_secret_version(name=name)\n",
        "                secrets_dic[secret_id] = response.payload.data.decode(\"UTF-8\")\n",
        "                print(f\"[SECRET SUCCESS ‚úÖ] '{secret_id}' le√≠do.\", flush=True)\n",
        "            except Exception as e:\n",
        "                warnings_int += 1\n",
        "                print(f\"[SECRET WARNING ‚ö†Ô∏è] No se pudo leer '{secret_id}': {e}\", flush=True)\n",
        "\n",
        "        elapsed = round(time.time() - t0, 2)\n",
        "        print(\"üîπüîπüîπ [METRICS üìä] Resumen üîπüîπüîπ\", flush=True)\n",
        "        print(f\"[METRICS INFO ‚ÑπÔ∏è] Secretos solicitados: {len(secrets_list)}\", flush=True)\n",
        "        print(f\"[METRICS INFO ‚ÑπÔ∏è] Advertencias: {warnings_int}\", flush=True)\n",
        "        print(f\"[END FINISHED ‚úÖ] Tiempo total: {elapsed} s\", flush=True)\n",
        "        return secrets_dic\n",
        "\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"[PROCESS ERROR ‚ùå] Fallo en secrets_get_dic: {e}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Vd0F6HID39Ta"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title IMPORTACI√ìN DE SECRETOS HS\n",
        "\n",
        "from common.dpm_GCP_ini import _ini_authenticate_API\n",
        "\n",
        "# Configuraci√≥n actualizada para incluir tanto el access token como el client secret\n",
        "config = {\n",
        "    \"project_id\": \"animum-dev-datawarehouse\",\n",
        "    \"ini_environment_identificated\": ini_environment_identificated,\n",
        "    \"json_keyfile_local\": GCP_json_keyfile_local,\n",
        "    \"json_keyfile_colab\": GCP_json_keyfile_colab,\n",
        "    \"json_keyfile_GCP_secret_id\": GCP_json_keyfile_GCP_secret_id,\n",
        "    \"secrets_list\": [\n",
        "        \"hs_datawarehouse_sensitive_acces_token\",    # Tu access token existente\n",
        "        \"hs_datawarehouse_sensitive_secret_key\"   # El nuevo client secret\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Obtener ambos secretos\n",
        "secrets_dic = secrets_get_dic(config)\n",
        "\n",
        "# Variables para HubSpot OAuth\n",
        "hs_datawarehouse_sensitive_acces_token = secrets_dic[\"hs_datawarehouse_sensitive_acces_token\"]\n",
        "hs_datawarehouse_sensitive_secret_key = secrets_dic[\"hs_datawarehouse_sensitive_secret_key\"]\n",
        "\n",
        "print(f\"[HUBSPOT CONFIG ‚ÑπÔ∏è] Access token loaded: {'‚úÖ' if hs_datawarehouse_sensitive_acces_token else '‚ùå'}\")\n",
        "print(f\"[HUBSPOT CONFIG ‚ÑπÔ∏è] Client secret loaded: {'‚úÖ' if hs_datawarehouse_sensitive_secret_key else '‚ùå'}\")\n",
        "\n",
        "# Si necesitas tambi√©n el Client ID, agr√©galo aqu√≠:\n",
        "# \"hs_datawarehouse_client_id\"        # Si tambi√©n necesitas almacenar el Client ID"
      ],
      "metadata": {
        "id": "RWMb99Ue3-gC",
        "outputId": "b6e5468c-406f-4ed0-bc18-586db3785e3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîπüîπüîπ [START ‚ñ∂Ô∏è] Lectura de secretos (in-memory) üîπüîπüîπ\n",
            "[AUTHENTICATION SUCCESS ‚úÖ] Credenciales obtenidas.\n",
            "[SECRET SUCCESS ‚úÖ] 'hs_datawarehouse_sensitive_acces_token' le√≠do.\n",
            "[SECRET SUCCESS ‚úÖ] 'hs_datawarehouse_sensitive_secret_key' le√≠do.\n",
            "üîπüîπüîπ [METRICS üìä] Resumen üîπüîπüîπ\n",
            "[METRICS INFO ‚ÑπÔ∏è] Secretos solicitados: 2\n",
            "[METRICS INFO ‚ÑπÔ∏è] Advertencias: 0\n",
            "[END FINISHED ‚úÖ] Tiempo total: 0.2 s\n",
            "[HUBSPOT CONFIG ‚ÑπÔ∏è] Access token loaded: ‚úÖ\n",
            "[HUBSPOT CONFIG ‚ÑπÔ∏è] Client secret loaded: ‚úÖ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title HS_to_GBQ_sensitive_data()\n",
        "# __________________________________________________________________________________________________________________________________________________________\n",
        "# HS_to_GBQ_sensitive_data\n",
        "# __________________________________________________________________________________________________________________________________________________________\n",
        "def HS_to_GBQ_sensitive_data(config: dict) -> None:\n",
        "    \"\"\"\n",
        "    Extrae contactos de HubSpot en chunks de hasta 10k contactos (paginando por createdate),\n",
        "    enriquece con propiedades sensibles v√≠a batch/read y carga cada chunk en BigQuery\n",
        "    a trav√©s de un CSV temporal en GCS. Autenticaci√≥n centralizada con _ini_authenticate_API().\n",
        "\n",
        "    Args:\n",
        "        config (dict):\n",
        "            # --- Autenticaci√≥n / Entorno ---\n",
        "            - ini_environment_identificated (str): \"LOCAL\" | \"COLAB\" | \"COLAB_ENTERPRISE\" | otro entorno GCP.\n",
        "            - json_keyfile_local (str, opcional): Ruta JSON si entorno LOCAL.\n",
        "            - json_keyfile_colab (str, opcional): Ruta JSON si entorno COLAB.\n",
        "            - json_keyfile_GCP_secret_id (str, opcional): SecretId con la JSON key si entorno GCP.\n",
        "            # --- Destinos GCP ---\n",
        "            - GBQ_project_id (str): ID del proyecto para BigQuery/Storage.\n",
        "            - GCS_bucket_name (str): Bucket temporal para CSVs.\n",
        "            - GBQ_dataset_id (str): Dataset destino.\n",
        "            - GBQ_table_id (str): Tabla destino.\n",
        "            # --- HubSpot ---\n",
        "            - HS_api_key (str): Token de Private App (o usa os.environ y p√°salo aqu√≠).\n",
        "            - HS_fields_no_sensitive_names_list (list[str]): Propiedades NO sensibles para /search (fuerza \"id\" y \"createdate\").\n",
        "            - HS_fields_sensitive_names_list (list[str]): Propiedades sensibles para /batch/read.\n",
        "            - HS_api_lines_per_call (int): L√≠mite por llamada a /search (<=100 es razonable).\n",
        "            - hs_contact_filter_createdate (dict): {\"from\": \"YYYY-MM-DD\", \"to\": \"YYYY-MM-DD|''\", \"mode\": \"between\"}.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "\n",
        "    Raises:\n",
        "        ValueError: Si faltan par√°metros obligatorios o el rango de fechas es inv√°lido.\n",
        "        Exception: Si falla la extracci√≥n/carga tras reintentos.\n",
        "    \"\"\"\n",
        "    # --------------------------- Importaciones locales (reducir dependencias globales) ---------------------------\n",
        "    import os\n",
        "    import io\n",
        "    import uuid\n",
        "    import time\n",
        "    import math\n",
        "    import csv\n",
        "    import json\n",
        "    import re\n",
        "    import pandas as pd\n",
        "    import requests\n",
        "    from datetime import datetime, timedelta, timezone\n",
        "    from google.cloud import bigquery, storage, secretmanager  # noqa: F401 (secretmanager no se usa aqu√≠)\n",
        "    # ------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "    # --------------------------- VALIDACI√ìN DE PAR√ÅMETROS -------------------------------------------------------\n",
        "    required_keys = [\n",
        "        \"GBQ_project_id\", \"GCS_bucket_name\", \"GBQ_dataset_id\", \"GBQ_table_id\",\n",
        "        \"HS_api_key\", \"HS_fields_no_sensitive_names_list\", \"HS_fields_sensitive_names_list\",\n",
        "        \"HS_api_lines_per_call\", \"hs_contact_filter_createdate\"\n",
        "    ]\n",
        "    missing = [k for k in required_keys if k not in config]\n",
        "    if missing:\n",
        "        raise ValueError(f\"[VALIDATION [ERROR ‚ùå]] Faltan claves en config: {missing}\")\n",
        "    # Validaciones de tipo m√≠nimas\n",
        "    if not isinstance(config[\"HS_fields_no_sensitive_names_list\"], list):\n",
        "        raise ValueError(\"[VALIDATION [ERROR ‚ùå]] 'HS_fields_no_sensitive_names_list' debe ser list.\")\n",
        "    if not isinstance(config[\"HS_fields_sensitive_names_list\"], list):\n",
        "        raise ValueError(\"[VALIDATION [ERROR ‚ùå]] 'HS_fields_sensitive_names_list' debe ser list.\")\n",
        "    if not isinstance(config[\"hs_contact_filter_createdate\"], dict):\n",
        "        raise ValueError(\"[VALIDATION [ERROR ‚ùå]] 'hs_contact_filter_createdate' debe ser dict.\")\n",
        "    # Reforzamos 'id' y 'createdate' en NO sensibles\n",
        "    if \"id\" not in config[\"HS_fields_no_sensitive_names_list\"]:\n",
        "        config[\"HS_fields_no_sensitive_names_list\"].append(\"id\")\n",
        "    if \"createdate\" not in config[\"HS_fields_no_sensitive_names_list\"]:\n",
        "        config[\"HS_fields_no_sensitive_names_list\"].append(\"createdate\")\n",
        "    print(\"üîπüîπüîπ [START ‚ñ∂Ô∏è] HS_to_GBQ_sensitive_data (extracci√≥n y carga) üîπüîπüîπ\", flush=True)  # :contentReference[oaicite:6]{index=6}\n",
        "\n",
        "    # --------------------------- PAR√ÅMETROS BASE ---------------------------------------------------------------\n",
        "    GBQ_project_id = config[\"GBQ_project_id\"]\n",
        "    GCS_bucket_name = config[\"GCS_bucket_name\"]\n",
        "    GBQ_dataset_id = config[\"GBQ_dataset_id\"]\n",
        "    GBQ_table_id = config[\"GBQ_table_id\"]\n",
        "    HS_api_key = config[\"HS_api_key\"]\n",
        "    hs_fields_no_sensitive = sorted(config[\"HS_fields_no_sensitive_names_list\"])\n",
        "    hs_fields_sensitive = sorted(config[\"HS_fields_sensitive_names_list\"])\n",
        "    HS_api_lines_per_call = int(config[\"HS_api_lines_per_call\"])\n",
        "    createdate_filter = config[\"hs_contact_filter_createdate\"]\n",
        "\n",
        "    # Fechas (acepta to == \"\" -> hoy 23:59:59)\n",
        "    try:\n",
        "        from_date = datetime.strptime(createdate_filter[\"from\"], \"%Y-%m-%d\").replace(tzinfo=timezone.utc)\n",
        "        if not createdate_filter.get(\"to\"):\n",
        "            to_date = datetime.now(timezone.utc).replace(hour=23, minute=59, second=59, microsecond=0)\n",
        "        else:\n",
        "            to_date = datetime.strptime(createdate_filter[\"to\"], \"%Y-%m-%d\").replace(\n",
        "                hour=23, minute=59, second=59, tzinfo=timezone.utc\n",
        "            )\n",
        "        if to_date < from_date:\n",
        "            raise ValueError(\"Rango de fechas inv√°lido: 'to' < 'from'.\")\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"[VALIDATION [ERROR ‚ùå]] Fechas inv√°lidas en 'hs_contact_filter_createdate': {e}\")  # :contentReference[oaicite:7]{index=7}\n",
        "\n",
        "    # --------------------------- AUTENTICACI√ìN CENTRALIZADA ----------------------------------------------------\n",
        "    print(\"[AUTHENTICATION START ‚ñ∂Ô∏è] Inicializando credenciales...\", flush=True)\n",
        "    credentials = _ini_authenticate_API(config, GBQ_project_id)  # usa ini_environment_identificated & json_keyfile_*  :contentReference[oaicite:8]{index=8}\n",
        "    bq_client = bigquery.Client(project=GBQ_project_id, credentials=credentials)\n",
        "    st_client = storage.Client(project=GBQ_project_id, credentials=credentials)\n",
        "    print(\"[AUTHENTICATION SUCCESS ‚úÖ] Credenciales listas.\", flush=True)\n",
        "\n",
        "    # --------------------------- AUXILIARES INTERNAS -----------------------------------------------------------\n",
        "    def _iso_z(dt: datetime) -> str:\n",
        "        return dt.astimezone(timezone.utc).replace(microsecond=0).isoformat().replace(\"+00:00\", \"Z\")\n",
        "\n",
        "    def _hubspot_search_chunk(start_dt: datetime, end_dt: datetime, limit_chunk: int = 10_000):\n",
        "        \"\"\"Devuelve (rows_list, last_createdate_dt, reached_limit_bool) con backoff b√°sico.\"\"\"\n",
        "        url = \"https://api.hubapi.com/crm/v3/objects/contacts/search\"\n",
        "        headers = {\"Authorization\": f\"Bearer {HS_api_key}\", \"Content-Type\": \"application/json\"}\n",
        "        accumulated, after = [], None\n",
        "        # bucle paginado hasta llenar el chunk\n",
        "        while len(accumulated) < limit_chunk:\n",
        "            remaining = limit_chunk - len(accumulated)\n",
        "            body = {\n",
        "                \"filterGroups\": [{\n",
        "                    \"filters\": [\n",
        "                        {\"propertyName\": \"createdate\", \"operator\": \"GTE\", \"value\": _iso_z(start_dt)},\n",
        "                        {\"propertyName\": \"createdate\", \"operator\": \"LTE\", \"value\": _iso_z(end_dt)}\n",
        "                    ]\n",
        "                }],\n",
        "                \"sorts\": [{\"propertyName\": \"createdate\", \"direction\": \"ASCENDING\"}],\n",
        "                \"properties\": hs_fields_no_sensitive,\n",
        "                \"limit\": min(HS_api_lines_per_call, remaining)\n",
        "            }\n",
        "            if after:\n",
        "                body[\"after\"] = after\n",
        "            # backoff simple\n",
        "            retry, max_retry, backoff = 0, 5, 1.2\n",
        "            while True:\n",
        "                resp = requests.post(url, headers=headers, json=body, timeout=60)\n",
        "                if resp.status_code == 200:\n",
        "                    break\n",
        "                retry += 1\n",
        "                if retry > max_retry:\n",
        "                    raise requests.HTTPError(f\"/search fallo tras {max_retry} reintentos: {resp.text}\")\n",
        "                time.sleep(backoff ** retry)\n",
        "            data = resp.json()\n",
        "            results = data.get(\"results\", [])\n",
        "            if not results:\n",
        "                break\n",
        "            for r in results:\n",
        "                props = r.get(\"properties\", {})\n",
        "                row = {c: (r.get(\"id\") if c == \"id\" else props.get(c)) for c in hs_fields_no_sensitive}\n",
        "                accumulated.append(row)\n",
        "            after = data.get(\"paging\", {}).get(\"next\", {}).get(\"after\")\n",
        "            if not after:\n",
        "                break\n",
        "\n",
        "        if not accumulated:\n",
        "            return [], None, False\n",
        "\n",
        "        last_cdate_str = accumulated[-1].get(\"createdate\")\n",
        "        try:\n",
        "            # HubSpot devuelve ISO con 'Z'\n",
        "            last_dt = datetime.fromisoformat(last_cdate_str.replace(\"Z\", \"+00:00\")).astimezone(timezone.utc) if last_cdate_str else None\n",
        "        except Exception:\n",
        "            last_dt = None\n",
        "        reached = (len(accumulated) >= limit_chunk)\n",
        "        return accumulated, last_dt, reached\n",
        "\n",
        "    def _hubspot_batch_sensitive(contact_rows: list) -> pd.DataFrame:\n",
        "        if not hs_fields_sensitive:\n",
        "            print(\"[PROCESSING INFO ‚ÑπÔ∏è] Sin propiedades sensibles; devolviendo NO sensibles.\", flush=True)\n",
        "            return pd.DataFrame(contact_rows)\n",
        "        if not contact_rows:\n",
        "            print(\"[PROCESSING INFO ‚ÑπÔ∏è] Lista vac√≠a de contactos; nada que enriquecer.\", flush=True)\n",
        "            return pd.DataFrame([])\n",
        "\n",
        "        ids = [r.get(\"id\") for r in contact_rows if r.get(\"id\")]\n",
        "        if not ids:\n",
        "            print(\"[PROCESSING WARNING ‚ö†Ô∏è] No hay IDs; se omite batch/read.\", flush=True)\n",
        "            return pd.DataFrame(contact_rows)\n",
        "\n",
        "        url_b = \"https://api.hubapi.com/crm/v3/objects/contacts/batch/read\"\n",
        "        headers = {\"Authorization\": f\"Bearer {HS_api_key}\", \"Content-Type\": \"application/json\"}\n",
        "\n",
        "        id_to_sens = {}\n",
        "        chunk_size = 100\n",
        "        total = len(ids)\n",
        "        n_chunks = math.ceil(total / chunk_size)\n",
        "        print(f\"[PROCESSING INFO ‚ÑπÔ∏è] Recuperando {len(hs_fields_sensitive)} propiedades sensibles para {total} IDs en {n_chunks} lotes.\", flush=True)\n",
        "\n",
        "        for i in range(n_chunks):\n",
        "            subset = ids[i*chunk_size : (i+1)*chunk_size]\n",
        "            body_b = {\"properties\": hs_fields_sensitive, \"inputs\": [{\"id\": cid} for cid in subset]}\n",
        "            # backoff simple\n",
        "            retry, max_retry, backoff = 0, 5, 1.2\n",
        "            while True:\n",
        "                resp_b = requests.post(url_b, headers=headers, json=body_b, timeout=60)\n",
        "                if resp_b.status_code == 200:\n",
        "                    break\n",
        "                retry += 1\n",
        "                if retry > max_retry:\n",
        "                    raise requests.HTTPError(f\"/batch/read fallo tras {max_retry} reintentos: {resp_b.text}\")\n",
        "                time.sleep(backoff ** retry)\n",
        "\n",
        "            data_b = resp_b.json()\n",
        "            for rb in data_b.get(\"results\", []):\n",
        "                c_id = rb.get(\"id\")\n",
        "                p_b = rb.get(\"properties\", {}) or {}\n",
        "                id_to_sens[c_id] = {s: p_b.get(s) for s in hs_fields_sensitive}\n",
        "            print(f\"[PROCESSING INFO ‚ÑπÔ∏è] Lote {i+1}/{n_chunks} procesado.\", flush=True)\n",
        "\n",
        "        df = pd.DataFrame(contact_rows)\n",
        "        for sfield in hs_fields_sensitive:\n",
        "            df[sfield] = df[\"id\"].apply(lambda cid: (id_to_sens.get(cid) or {}).get(sfield))\n",
        "        return df\n",
        "\n",
        "    def _upload_df_to_bq(df: pd.DataFrame, disposition: str = \"WRITE_APPEND\"):\n",
        "        if df.empty:\n",
        "            print(\"[LOAD WARNING ‚ö†Ô∏è] DataFrame vac√≠o; se omite carga.\", flush=True)\n",
        "            return 0\n",
        "\n",
        "        # CSV temporal local -> GCS -> BQ\n",
        "        tmp_local = f\"temp_contacts_{uuid.uuid4().hex}.csv\"\n",
        "        df.to_csv(tmp_local, index=False, quoting=csv.QUOTE_MINIMAL)\n",
        "        blob_name = f\"tmp_hs/contacts_{uuid.uuid4().hex}.csv\"\n",
        "\n",
        "        try:\n",
        "            bucket = st_client.bucket(GCS_bucket_name)\n",
        "            blob = bucket.blob(blob_name)\n",
        "            print(f\"[LOAD INFO ‚ÑπÔ∏è] Subiendo CSV temporal a gs://{GCS_bucket_name}/{blob_name} ...\", flush=True)\n",
        "            blob.upload_from_filename(tmp_local)\n",
        "\n",
        "            table_id = f\"{GBQ_project_id}.{GBQ_dataset_id}.{GBQ_table_id}\"\n",
        "            job_config = bigquery.LoadJobConfig(\n",
        "                write_disposition=disposition,\n",
        "                source_format=bigquery.SourceFormat.CSV,\n",
        "                autodetect=True,\n",
        "                field_delimiter=\",\",\n",
        "                quote_character='\"',\n",
        "                allow_quoted_newlines=True,\n",
        "                labels={\"source\": \"hubspot\", \"pipeline\": \"hs_sensitive\", \"mode\": \"batch\"}\n",
        "            )\n",
        "            print(f\"[LOAD START ‚ñ∂Ô∏è] Cargando en BigQuery => {table_id} ...\", flush=True)\n",
        "            load_job = bq_client.load_table_from_uri(\n",
        "                f\"gs://{GCS_bucket_name}/{blob_name}\",\n",
        "                table_id,\n",
        "                job_config=job_config,\n",
        "            )\n",
        "            load_job.result()\n",
        "            rows_loaded = len(df)\n",
        "            print(f\"[LOAD SUCCESS ‚úÖ] Carga completada ({rows_loaded} filas).\", flush=True)\n",
        "            return rows_loaded\n",
        "        finally:\n",
        "            # Limpieza robusta\n",
        "            try:\n",
        "                if 'blob' in locals():\n",
        "                    blob.delete()\n",
        "            except Exception:\n",
        "                print(\"[LOAD WARNING ‚ö†Ô∏è] No se pudo eliminar el blob temporal en GCS.\", flush=True)\n",
        "            try:\n",
        "                if os.path.exists(tmp_local):\n",
        "                    os.remove(tmp_local)\n",
        "            except Exception:\n",
        "                print(\"[LOAD WARNING ‚ö†Ô∏è] No se pudo eliminar el CSV temporal local.\", flush=True)\n",
        "\n",
        "    # --------------------------- LOOP PRINCIPAL POR CHUNKS ------------------------------------------------------\n",
        "    total_rows, total_chunks = 0, 0\n",
        "    t0 = time.time()\n",
        "    current_start = from_date\n",
        "    print(f\"[PROCESSING INFO ‚ÑπÔ∏è] Ventana: {from_date} ‚Üí {to_date}\", flush=True)\n",
        "    print(f\"[PROCESSING INFO ‚ÑπÔ∏è] NO sensibles: {hs_fields_no_sensitive}\", flush=True)\n",
        "    print(f\"[PROCESSING INFO ‚ÑπÔ∏è] Sensibles: {hs_fields_sensitive}\", flush=True)\n",
        "\n",
        "    while current_start <= to_date:\n",
        "        total_chunks += 1\n",
        "        print(f\"--- Chunk #{total_chunks} | desde {current_start.isoformat()}Z ---\", flush=True)\n",
        "\n",
        "        rows, last_dt, reached = _hubspot_search_chunk(current_start, to_date, limit_chunk=10_000)\n",
        "        if not rows:\n",
        "            print(\"[PROCESSING INFO ‚ÑπÔ∏è] Sin resultados en este tramo; fin del proceso.\", flush=True)\n",
        "            break\n",
        "\n",
        "        print(f\"[PROCESSING INFO ‚ÑπÔ∏è] {len(rows)} contactos base. Enriqueciendo sensibles...\", flush=True)\n",
        "        df_chunk = _hubspot_batch_sensitive(rows)\n",
        "\n",
        "        # Primera carga: TRUNCATE. Siguientes: APPEND.\n",
        "        disposition = \"WRITE_TRUNCATE\" if total_chunks == 1 else \"WRITE_APPEND\"\n",
        "        total_rows += _upload_df_to_bq(df_chunk, disposition=disposition)\n",
        "\n",
        "        if not last_dt:\n",
        "            print(\"[PROCESSING WARNING ‚ö†Ô∏è] 'last createdate' indeterminado; se detiene paginaci√≥n.\", flush=True)\n",
        "            break\n",
        "        # Avanza 1 segundo tras el √∫ltimo 'createdate' paginado\n",
        "        current_start = last_dt + timedelta(seconds=1)\n",
        "\n",
        "        if not reached:\n",
        "            print(\"[PROCESSING INFO ‚ÑπÔ∏è] No se alcanz√≥ el tope de 10k; no hay m√°s contactos en rango.\", flush=True)\n",
        "            break\n",
        "\n",
        "    # --------------------------- M√âTRICAS FINALES ---------------------------------------------------------------\n",
        "    elapsed = round(time.time() - t0, 2)\n",
        "    print(\"üîπüîπüîπ [METRICS üìä] Resumen de ejecuci√≥n üîπüîπüîπ\", flush=True)  # :contentReference[oaicite:9]{index=9}\n",
        "    print(f\"[METRICS INFO ‚ÑπÔ∏è] Proyecto: {GBQ_project_id}\", flush=True)\n",
        "    print(f\"[METRICS INFO ‚ÑπÔ∏è] Dataset.Tabla: {GBQ_dataset_id}.{GBQ_table_id}\", flush=True)\n",
        "    print(f\"[METRICS INFO ‚ÑπÔ∏è] Chunks procesados: {total_chunks}\", flush=True)\n",
        "    print(f\"[METRICS INFO ‚ÑπÔ∏è] Filas cargadas: {total_rows}\", flush=True)\n",
        "    print(f\"[END FINISHED ‚úÖ] Tiempo total: {elapsed} s\", flush=True)\n"
      ],
      "metadata": {
        "id": "92qRakm85joV"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EJECUCIONES"
      ],
      "metadata": {
        "id": "SDY2ZxTa_G-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title IMPORTACI√ìN DATOS SENSIBLES HS TO GBQ\n",
        "config = {\n",
        "    \"ini_environment_identificated\": ini_environment_identificated,\n",
        "    \"json_keyfile_local\": GCP_json_keyfile_local,\n",
        "    \"json_keyfile_colab\": GCP_json_keyfile_colab,\n",
        "    \"json_keyfile_GCP_secret_id\": GCP_json_keyfile_GCP_secret_id,\n",
        "\n",
        "    \"GBQ_project_id\": \"animum-dev-datawarehouse\",\n",
        "    \"GCS_bucket_name\": \"temp_datawarehouse\",\n",
        "    \"GBQ_dataset_id\": \"tp_02st_01\",\n",
        "    \"GBQ_table_id\": \"hs_contact_sensitive_cleaned\",\n",
        "\n",
        "    \"HS_api_key\": HS_api_key_str,\n",
        "    \"HS_fields_no_sensitive_names_list\": [\"email\"],\n",
        "    \"HS_fields_sensitive_names_list\": [\"iban\",\"codigo_bic_swift\",\"documento_nacional_de_identidad_numero\"],\n",
        "    \"HS_api_lines_per_call\": 100,\n",
        "    \"hs_contact_filter_createdate\": {\"from\": \"2025-01-01\", \"to\": \"2025-08-29\", \"mode\": \"between\"}\n",
        "}\n",
        "\n",
        "HS_to_GBQ_sensitive_data(config)"
      ],
      "metadata": {
        "id": "nNW5r7wZ9b1r",
        "outputId": "e5d68461-15cb-43e3-97a0-866ec9201807",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîπüîπüîπ [START ‚ñ∂Ô∏è] HS_to_GBQ_sensitive_data (extracci√≥n y carga) üîπüîπüîπ\n",
            "[AUTHENTICATION START ‚ñ∂Ô∏è] Inicializando credenciales...\n",
            "[AUTHENTICATION SUCCESS ‚úÖ] Credenciales listas.\n",
            "[PROCESSING INFO ‚ÑπÔ∏è] Ventana: 2025-01-01 00:00:00+00:00 ‚Üí 2025-08-29 23:59:59+00:00\n",
            "[PROCESSING INFO ‚ÑπÔ∏è] NO sensibles: ['createdate', 'email', 'id']\n",
            "[PROCESSING INFO ‚ÑπÔ∏è] Sensibles: ['codigo_bic_swift', 'documento_nacional_de_identidad_numero', 'iban']\n",
            "--- Chunk #1 | desde 2025-01-01T00:00:00+00:00Z ---\n",
            "[PROCESSING INFO ‚ÑπÔ∏è] 6768 contactos base. Enriqueciendo sensibles...\n",
            "[PROCESSING INFO ‚ÑπÔ∏è] Recuperando 3 propiedades sensibles para 6768 IDs en 68 lotes.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "HTTPError",
          "evalue": "/batch/read fallo tras 5 reintentos: {\"status\":\"error\",\"message\":\"This app hasn't been granted all required scopes to make this call. Read more about required scopes here: https://developers.hubspot.com/scopes.\",\"correlationId\":\"d8f2dacb-6f75-40e4-958e-8b46812ba7dc\",\"errors\":[{\"message\":\"One or more of the following scopes are required.\",\"context\":{\"requiredGranularScopes\":[\"crm.objects.contacts.sensitive.read.v2\",\"crm.objects.contacts.highly_sensitive.read.v2\"]}}],\"links\":{\"scopes\":\"https://developers.hubspot.com/scopes\"},\"category\":\"MISSING_SCOPES\"}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2391750678.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m }\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mHS_to_GBQ_sensitive_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3566355978.py\u001b[0m in \u001b[0;36mHS_to_GBQ_sensitive_data\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[PROCESSING INFO ‚ÑπÔ∏è] {len(rows)} contactos base. Enriqueciendo sensibles...\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0mdf_chunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_hubspot_batch_sensitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;31m# Primera carga: TRUNCATE. Siguientes: APPEND.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3566355978.py\u001b[0m in \u001b[0;36m_hubspot_batch_sensitive\u001b[0;34m(contact_rows)\u001b[0m\n\u001b[1;32m    197\u001b[0m                 \u001b[0mretry\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mretry\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmax_retry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"/batch/read fallo tras {max_retry} reintentos: {resp_b.text}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackoff\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mretry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: /batch/read fallo tras 5 reintentos: {\"status\":\"error\",\"message\":\"This app hasn't been granted all required scopes to make this call. Read more about required scopes here: https://developers.hubspot.com/scopes.\",\"correlationId\":\"d8f2dacb-6f75-40e4-958e-8b46812ba7dc\",\"errors\":[{\"message\":\"One or more of the following scopes are required.\",\"context\":{\"requiredGranularScopes\":[\"crm.objects.contacts.sensitive.read.v2\",\"crm.objects.contacts.highly_sensitive.read.v2\"]}}],\"links\":{\"scopes\":\"https://developers.hubspot.com/scopes\"},\"category\":\"MISSING_SCOPES\"}"
          ]
        }
      ]
    }
  ]
}